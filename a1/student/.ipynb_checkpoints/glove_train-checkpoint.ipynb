{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5506fa66-3250-4a2f-b211-12cdbaebeed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abaca96b-378b-4219-addb-e3b9dd916706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    # 移除标点符号和数字\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    # 去除停用词\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "# 读取并清洗语料库\n",
    "corpus_path = \"/home/jia/PycharmProjects/CS224n/GloVe/text8\"\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "cleaned_tokens = clean_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac452c6-3f8d-40bf-8ae3-54334dad06ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'term', 'abuse', 'first']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8253a98c-fc03-437d-8a11-a7932a029c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_freq = defaultdict(int)\n",
    "for word in cleaned_tokens:\n",
    "    word_freq[word] += 1\n",
    "\n",
    "min_freq = 5\n",
    "filtered_words = [word for word in cleaned_tokens if word_freq[word] >= min_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be5ac61-4190-407d-8398-d137864c1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词汇表\n",
    "vocab = list(set(filtered_words))\n",
    "word2id = {word: idx for idx, word in enumerate(vocab)}\n",
    "id2word = {idx: word for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a0c1d1-d5de-4c0f-b4d9-851f8cfc3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10602003/10602003 [01:25<00:00, 124541.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "window_size = 5  # 上下文窗口大小\n",
    "cooccur = defaultdict(lambda: defaultdict(float))\n",
    "# 遍历每个中心词\n",
    "for center_pos in tqdm(range(len(filtered_words))):\n",
    "    center_word = filtered_words[center_pos]\n",
    "    center_id = word2id[center_word]\n",
    "    \n",
    "    # 遍历窗口内的上下文\n",
    "    start = max(0, center_pos - window_size)\n",
    "    end = min(len(filtered_words), center_pos + window_size + 1)\n",
    "    \n",
    "    for context_pos in range(start, end):\n",
    "        if context_pos == center_pos:\n",
    "            continue  # 跳过中心词本身\n",
    "        context_word = filtered_words[context_pos]\n",
    "        context_id = word2id[context_word]\n",
    "        \n",
    "        # 根据距离加权 (可选: 1/distance)\n",
    "        distance = abs(context_pos - center_pos)\n",
    "        cooccur[center_id][context_id] += 1.0 / distance\n",
    "# 转换为稀疏矩阵格式 (i, j, X_ij)\n",
    "rows, cols, values = [], [], []\n",
    "for i in cooccur:\n",
    "    for j in cooccur[i]:\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        values.append(cooccur[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ceace2-a625-4b19-afe1-af99731001f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch 版本训练\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "class GloveDataset(Dataset):\n",
    "    def __init__(self, coo_matrix):\n",
    "        self.rows = torch.LongTensor(coo_matrix.row)\n",
    "        self.cols = torch.LongTensor(coo_matrix.col)\n",
    "        self.values = torch.FloatTensor(coo_matrix.data)\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.rows[idx], self.cols[idx], self.values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97002fbe-c63a-483a-a26c-edced1e79ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVePyTorch(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, x_max=100, alpha=0.75):\n",
    "        super().__init__()\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # 词向量和偏置项（中心词和上下文词分开）\n",
    "        self.w = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.w_tilde = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.b = nn.Embedding(vocab_size, 1)\n",
    "        self.b_tilde = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "        # 初始化参数\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.w.weight.data.uniform_(-init_range, init_range)\n",
    "        self.w_tilde.weight.data.uniform_(-init_range, init_range)\n",
    "        self.b.weight.data.zero_()\n",
    "        self.b_tilde.weight.data.zero_()\n",
    "        \n",
    "    def forward(self, i, j, X_ij):\n",
    "        # 计算加权损失\n",
    "        weights = (X_ij / self.x_max) ** self.alpha\n",
    "        weights = torch.clamp(weights, max=1.0)\n",
    "        \n",
    "        # 计算内积和偏置\n",
    "        w_i = self.w(i)\n",
    "        w_tilde_j = self.w_tilde(j)\n",
    "        b_i = self.b(i).squeeze()\n",
    "        b_tilde_j = self.b_tilde(j).squeeze()\n",
    "        \n",
    "        similarity = torch.sum(w_i * w_tilde_j, dim=1)\n",
    "        preds = similarity + b_i + b_tilde_j\n",
    "        logs = torch.log(X_ij)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = torch.mean(weights * (preds - logs) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2983c54c-3cef-446d-ac96-437057022474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "def train_glove(coo_mat, vocab_size, device='cuda', batch_size=163840, epochs=50):\n",
    "    # 准备数据集\n",
    "    dataset = GloveDataset(coo_mat)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers=16,pin_memory=True)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = GloVePyTorch(vocab_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # 新增混合精度训练组件\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, \n",
    "                        desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "                        leave=True)\n",
    "        for i, j, X_ij in progress_bar:\n",
    "            i = i.to(device, non_blocking=True)\n",
    "            j = j.to(device, non_blocking=True)\n",
    "            X_ij = X_ij.to(device, non_blocking=True)\n",
    "            # i, j, X_ij = [x.to(device) for x in batch]\n",
    "            \n",
    "            \n",
    "            # optimizer.zero_grad()\n",
    "            # loss = model(i, j, X_ij)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                loss = model(i, j, X_ij)\n",
    "            # 梯度缩放与反向传播\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)  # 快速梯度清零\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f2c1ea-3581-4f34-a028-5f1d455b18ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|█████████████████████████████| 213/213 [07:32<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|█████████████████████████████| 213/213 [07:39<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Loss: 0.0445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|█████████████████████████████| 213/213 [07:34<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Loss: 0.0306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|█████████████████████████████| 213/213 [07:40<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Loss: 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|█████████████████████████████| 213/213 [07:28<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Loss: 0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|█████████████████████████████| 213/213 [07:42<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Loss: 0.0247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|█████████████████████████████| 213/213 [07:35<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Loss: 0.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|█████████████████████████████| 213/213 [07:40<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Loss: 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|█████████████████████████████| 213/213 [07:39<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Loss: 0.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|████████████████████████████| 213/213 [07:42<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|████████████████████████████| 213/213 [07:48<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Loss: 0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|████████████████████████████| 213/213 [07:54<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Loss: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|████████████████████████████| 213/213 [07:58<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Loss: 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|████████████████████████████| 213/213 [07:49<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|████████████████████████████| 213/213 [07:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|████████████████████████████| 213/213 [07:56<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Loss: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|████████████████████████████| 213/213 [07:55<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Loss: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|████████████████████████████| 213/213 [07:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|████████████████████████████| 213/213 [07:57<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|████████████████████████████| 213/213 [07:40<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|████████████████████████████| 213/213 [07:40<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | Loss: 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|████████████████████████████| 213/213 [07:49<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|████████████████████████████| 213/213 [07:44<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|████████████████████████████| 213/213 [07:42<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | Loss: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|████████████████████████████| 213/213 [07:44<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|████████████████████████████| 213/213 [07:40<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|████████████████████████████| 213/213 [07:34<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|████████████████████████████| 213/213 [07:49<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|████████████████████████████| 213/213 [07:51<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|████████████████████████████| 213/213 [07:44<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|████████████████████████████| 213/213 [07:55<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|████████████████████████████| 213/213 [07:43<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|████████████████████████████| 213/213 [07:38<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|████████████████████████████| 213/213 [07:52<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|████████████████████████████| 213/213 [07:43<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|████████████████████████████| 213/213 [07:45<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|████████████████████████████| 213/213 [07:49<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 | Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|████████████████████████████| 213/213 [07:36<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 | Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|████████████████████████████| 213/213 [07:40<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 | Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|████████████████████████████| 213/213 [07:58<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 | Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|████████████████████████████| 213/213 [07:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 | Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|████████████████████████████| 213/213 [07:46<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 | Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|████████████████████████████| 213/213 [07:46<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 | Loss: 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|████████████████████████████| 213/213 [07:46<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 | Loss: 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|████████████████████████████| 213/213 [07:43<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 | Loss: 0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|████████████████████████████| 213/213 [07:51<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 | Loss: 0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|████████████████████████████| 213/213 [07:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 | Loss: 0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|████████████████████████████| 213/213 [07:45<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 | Loss: 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|████████████████████████████| 213/213 [07:50<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 | Loss: 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|████████████████████████████| 213/213 [07:44<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 | Loss: 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "coo = coo_matrix((values, (rows, cols)), shape=(len(vocab), len(vocab)))\n",
    "# 训练并保存模型\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = train_glove(coo, len(vocab), device=device)\n",
    "# 合并词向量（中心词和上下文词平均）\n",
    "final_embeddings = (model.w.weight.data + model.w_tilde.weight.data) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc346914-5c47-4d10-b78e-c59f6b9599cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存到文件\n",
    "with open(\"glove_pytorch.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, idx in word2id.items():\n",
    "        vec = final_embeddings[idx].cpu().numpy()\n",
    "        f.write(f\"{word} \" + \" \".join(map(str, vec)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444afc64-943c-4550-968b-6352e0764416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# from scipy.sparse import coo_matrix\n",
    "\n",
    "# class GloVeManual:\n",
    "#     def __init__(self, vector_dim=100, x_max=100, alpha=0.75, lr=0.05):\n",
    "#         self.vector_dim = vector_dim  # 词向量维度\n",
    "#         self.x_max = x_max            # 权重函数截断值\n",
    "#         self.alpha = alpha            # 权重函数指数\n",
    "#         self.lr = lr                  # 学习率\n",
    "        \n",
    "#     def _init_params(self, vocab_size):\n",
    "#         # 初始化中心词和上下文词向量\n",
    "#         self.W = np.random.randn(vocab_size, self.vector_dim) * 0.1\n",
    "#         self.W_tilde = np.random.randn(vocab_size, self.vector_dim) * 0.1\n",
    "        \n",
    "#         # 初始化偏置项\n",
    "#         self.b = np.zeros(vocab_size)\n",
    "#         self.b_tilde = np.zeros(vocab_size)\n",
    "        \n",
    "#     def _weight_func(self, x):\n",
    "#         # 定义权重函数 f(x)\n",
    "#         return np.minimum((x / self.x_max) ** self.alpha, 1.0)\n",
    "    \n",
    "#     def fit(self, coo_matrix, epochs=50):\n",
    "#         rows = coo_matrix.row\n",
    "#         cols = coo_matrix.col\n",
    "#         values = coo_matrix.data\n",
    "        \n",
    "#         vocab_size = coo_matrix.shape[0]\n",
    "#         self._init_params(vocab_size)\n",
    "        \n",
    "#         for epoch in range(epochs):\n",
    "#             total_loss = 0.0\n",
    "#             for idx in np.random.permutation(len(values)):  # 随机顺序遍历\n",
    "#                 i = rows[idx]\n",
    "#                 j = cols[idx]\n",
    "#                 X_ij = values[idx]\n",
    "                \n",
    "#                 # 计算预测值\n",
    "#                 pred = np.dot(self.W[i], self.W_tilde[j]) + self.b[i] + self.b_tilde[j]\n",
    "#                 log_X = np.log(X_ij)\n",
    "                \n",
    "#                 # 计算损失项\n",
    "#                 loss = pred - log_X\n",
    "#                 weighted_loss = self._weight_func(X_ij) * loss\n",
    "#                 total_loss += 0.5 * (weighted_loss * loss)\n",
    "                \n",
    "#                 # 计算梯度\n",
    "#                 grad_W_i = weighted_loss * self.W_tilde[j]\n",
    "#                 grad_W_tilde_j = weighted_loss * self.W[i]\n",
    "#                 grad_b_i = weighted_loss\n",
    "#                 grad_b_tilde_j = weighted_loss\n",
    "                \n",
    "#                 # 更新参数\n",
    "#                 self.W[i] -= self.lr * grad_W_i\n",
    "#                 self.W_tilde[j] -= self.lr * grad_W_tilde_j\n",
    "#                 self.b[i] -= self.lr * grad_b_i\n",
    "#                 self.b_tilde[j] -= self.lr * grad_b_tilde_j\n",
    "                \n",
    "#             print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.2f}\")\n",
    "            \n",
    "#     def get_vectors(self):\n",
    "#         # 合并中心词和上下文词向量（常见做法）\n",
    "#         return (self.W + self.W_tilde) / 2\n",
    "# coo = coo_matrix((values, (rows, cols)), shape=(len(vocab), len(vocab)))\n",
    "# # 初始化模型\n",
    "# glove = GloVeManual(vector_dim=100, x_max=100, alpha=0.75, lr=0.05)  # 100 维向量\n",
    "# glove.fit(coo)\n",
    "# # 获取最终词向量\n",
    "# word_vectors = glove.get_vectors()\n",
    "\n",
    "# # 保存到文件\n",
    "# with open(\"glove_manual_vectors.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for word in word2id:\n",
    "#         idx = word2id[word]\n",
    "#         vec = word_vectors[idx]\n",
    "#         f.write(f\"{word} \" + \" \".join(map(str, vec)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "436dd32d-6607-4c2c-9cdc-4f6db7e8d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6044898999818478\n"
     ]
    }
   ],
   "source": [
    "# 加载词向量\n",
    "def load_vectors(file_path):\n",
    "    word_vecs = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vec = np.array([float(x) for x in parts[1:]])\n",
    "            word_vecs[word] = vec\n",
    "    return word_vecs\n",
    "vectors = load_vectors(\"glove_pytorch.txt\")\n",
    "# 计算余弦相似度\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "print(cosine_similarity(vectors['king'], vectors['queen']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
